{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "**Word embedding** es el nombre de un conjunto de técnicas de modelado del lenguaje dadas dentro del Procesamiento del Lenguaje Natural (PLN), o *Natural Language Processing* en inglés, dónde las palabras o frases del vocabulario son vinculadas a vectores de números reales. Conceptualmente implica el encaje matemático de un espacio con una dimensión por palabra a un espacio vectorial continuo con menos dimensiones.\n",
    "\n",
    "Algunos de los métodos para generar este mapeo son redes neuronales, reducción de dimensionalidad en la matriz de co-ocurrencia de palabras, modelos probabilísticos, y representación explícita en términos del contexto en el cual estas palabras figuran.\n",
    "\n",
    "El *Word* y *phrase embeddings* (para palabras y frases respectivamente), utilizados de forma subyacente como forma de representación, demostraron aumentar el rendimiento de tareas en el procesamiento del lenguaje natural como en el análisis sintáctico y el análisis de sentimiento.\n",
    "\n",
    "## Vector one-hot\n",
    "\n",
    "Un vector *one-hot* consiste en un vector dónde sólo una de las posiciones tiene un valor, en este caso 1. Con él podemos representar un diccionario, donde cada palabra diferente se corresponde con un vector one-hot con una entrada diferente igual a 1.\n",
    "\n",
    "### Ejemplo con *embedding* del tamaño del diccionario\n",
    "\n",
    "En el siguiente ejemplo se codifican la palabras, dentro de un listado de frases, con un número entero equivalente a la entrada en el vector one-hot. El tamaño del vector debería ser igual al tamaño del diccionario. Sin embargo la función `one-hot()` de la librería `keras` no garantiza un número diferente para cada palabra, debido al método de hashing utilizado internamente, y por eso se usa un tamaño de vector mayor al número de palabras diferentes existentes para evitar colisiones.\n",
    "\n",
    "Luego, las frases codificadas de esta manera, y con relleno, se pasan a una red neuronal que aprende a clasificarlas según nuestro propio etiquetado de frases positivas o negativas. La precisión obtenida es aproximadamente del 80%, pero que puede variar ya que la función `one-hot()` no devuelve siempre los mismos resultados (además de existir otros procesos aleatorios como la inicialización de pesos de la red). Véase: https://keras.io/preprocessing/text/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39, 33], [18, 15], [36, 46], [25, 15], [32], [12], [43, 46], [23, 18], [43, 15], [21, 48, 33, 42]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "# define class labels. '1' means positive, '0' means negative.\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39 33  0  0]\n",
      " [18 15  0  0]\n",
      " [36 46  0  0]\n",
      " [25 15  0  0]\n",
      " [32  0  0  0]\n",
      " [12  0  0  0]\n",
      " [43 46  0  0]\n",
      " [23 18  0  0]\n",
      " [43 15  0  0]\n",
      " [21 48 33 42]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "El *embedding* utilizado en el ejercicio anterior parece bastante mejorable. Por un lado el tamaño del mismo puede ser excesivo si tenemos un diccionario grande, ya que hay una entrada por cada palabra. ¿Se podría tener una representación más compacta? Por otro la política aleatoria para su inicialización (asignación de una palabra a una entrada del vector) es seguramente mejorable. ¿Existen políticas mejores donde la posición de las palabras dentro del vector tenga algún significado?\n",
    "\n",
    "Con las técnicas **Word2vec** podemos conseguir un mejor *embedding* entrenando una sencilla red neuronal de dos capas con pares de palabras relacionadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación práctica\n",
    "\n",
    "Aquí se puede leer el tutorial entero: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "Los datos para entrenar nuestro modelo se pueden sacar fácilmente del texto disponible. Definimos un tamaño de ventana (un valor típico es 5) dentro del cual suponemos que hay relación entre las palabras y generamos todas las parejas posibles dentro de esa ventana. En la siguiente imagen se puede ver un ejemplo con tamaño de ventana 2.\n",
    "\n",
    "<img src=\"images/training_data.png\" width=\"60%\"/>\n",
    "\n",
    "Si tenemos un diccionario de 10.000 palabras la arquitectura de nuestra sencilla red neuronal sería como se ve en la imagen: vectores one-hot de tamaño 10.000 a la entrada, una única capa oculta de 300 neuronas (por ejemplo) y una capa de salida de 10.000 neuronas usando Softmax. Los valores de entrada serán el primer miembro de las parejas generadas anteriormente y el valor deseado el segundo miembro.\n",
    "\n",
    "<img src=\"images/skip_gram_net_arch.png\" width=\"80%\"/>\n",
    "\n",
    "Lo que conseguimos con esta arquitectura es que los pesos de la capa oculta, formada por las 300 neuronas, representen 300 características del diccionario. Los pesos aprendidos serán pues nuestro nuevo *embedding*.\n",
    "\n",
    "<img src=\"images/word2vec_weight_matrix_lookup_table.png\" width=\"50%\"/>\n",
    "\n",
    "A continuación se puede ver con un ejemplo reducido, como pasamos de una representación inicial de 5 valores (u vector one-hot de tamaño 5) a una representación compacta de sólo 3 valores. Esto se consigue gracias al entrenamiento de los pesos de una red oculta de 3 neuronas.\n",
    "\n",
    "<img src=\"images/matrix_mult_w_one_hot.png\" width=\"50%\"/>\n",
    "\n",
    "La salida de la red serán las probabilidades de que una palabra del diccionario esté relacionada con una palabra de entrada.\n",
    "\n",
    "<img src=\"images/output_weights_function.png\" width=\"80%\"/>\n",
    "\n",
    "En este enlace se pueden hacer consultas, sobre modelos previamente entrenados de finlandés, suomi e inglés, acerca de la relación entre palabras: http://bionlp-www.utu.fi/wv_demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnicas *Continuous Bag-Of-Words* (CBOW) y *Continuous Skip-gram*\n",
    "\n",
    "En Word2vec se pueden usar dos arquitecturas distintas para producir el *word embedding*:\n",
    "\n",
    "1. **Continuous Bag-Of-Words** (CBOW): \n",
    " - El modelo predice la palabra actual a partir de una ventana circundante de palabras del contexto.\n",
    " - El orden de las palabras del contexto no influye en la predicción.\n",
    " \n",
    "2. **Continuous Skip-gram**:\n",
    " - El modelo usa la palabra actual para predecir la ventana circundante de palabras del contexto.\n",
    " - Las palabras del contexto más cercanas tienen más peso que las palabras más distantes.\n",
    " \n",
    "*CBOW* es más rápido pero *Skip-gram* consigue mejores resultados para palabras poco frecuentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrización\n",
    "\n",
    "Los resultados de word2vec pueden ser sensibles a la parametrización. Algunos parámetros importantes son: \n",
    "\n",
    "* **Algoritmo de entrenamiento**: *Softmax jerárquico*, mejor para palabras poco frecuentes, o *Negative sampling*, mejor para palabras frecuentes y para vectores con menos dimensiones. A medida que el número de épocas aumenta el *Softmax jerárquico* deja de ser útil.\n",
    "\n",
    "* **Sub-sampling de palabras frecuentes**: Las palabras muy frecuentes a menudo aportan poca información. Las palabras con una frecuencia de aparición por encima de un cierto umbral pueden ser sub-muestreadas para aumentar la velocidad de entrenamiento. También puede mejorar la precisión en datasets grandes. Valores útiles están en el rango 0,001 y 0,00001. \n",
    "\n",
    "* **Dimensionalidad del word embedding**: La calidad del *word embedding* suele incrementarse a mayor dimensión del vector, que se corresponde con el número de neuronas de la capa oculta. Pero a partir de cierto punto la ganancia es poco significativa. Valores típicos para la dimensionalidad están entre 100 y 1000.\n",
    "\n",
    "* **Tamaño de la Ventana del contexto**: El tamaño de la ventana del contexto determina cuántas palabras antes y después de una palabra determinada deben ser incluídas dentro de su contexto. Valores recomendados son 10 para skip-gram y 5 para CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
    "\n",
    "https://github.com/stanfordnlp/GloVe\n",
    "\n",
    "The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. This simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. For example, man may be regarded as similar to woman in that both words describe human beings; on the other hand, the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.\n",
    "\n",
    "In order to capture in a quantitative way the nuance necessary to distinguish man from woman, it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo con GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "\n",
    "# define class labels. '1' means positive, '0' means negative.\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './GloVe/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e74f3829b5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the whole embedding into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./GloVe/glove.6B.100d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './GloVe/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./GloVe/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
